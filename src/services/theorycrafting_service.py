"""
Theorycrafting Logic System for Research Topic Assistance

This service provides intelligent research assistance by:
1. Analyzing user discussions and topics
2. Generating theoretical frameworks and hypotheses
3. Suggesting research directions and methodologies
4. Connecting disparate concepts and finding patterns
5. Providing evidence-based insights and critiques
"""
import asyncio
import json
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from datetime import datetime
from loguru import logger
import re
from collections import defaultdict, Counter
import math

try:
    from research_synthesis.database.connection import get_postgres_session
    from research_synthesis.database.models import Article, Entity, Report
    from sqlalchemy import select, and_, or_, func
    DATABASE_AVAILABLE = True
except ImportError:
    logger.warning("Database not available for theorycrafting service")
    DATABASE_AVAILABLE = False

try:
    from research_synthesis.services.ml_source_recommender import MLSourceRecommender
    ML_RECOMMENDER_AVAILABLE = True
except ImportError:
    logger.warning("ML Source Recommender not available for theorycrafting service")
    ML_RECOMMENDER_AVAILABLE = False

@dataclass
class ResearchConcept:
    """Represents a research concept or theory"""
    name: str
    description: str
    domain: str
    confidence_score: float
    evidence_count: int
    related_concepts: List[str]
    research_gaps: List[str]
    theoretical_foundation: str
    practical_applications: List[str]
    contradicting_evidence: List[str]

@dataclass
class ResearchHypothesis:
    """Represents a research hypothesis generated by theorycrafting"""
    hypothesis: str
    supporting_evidence: List[str]
    methodology_suggestions: List[str]
    expected_outcomes: List[str]
    risk_factors: List[str]
    confidence_level: float
    testability_score: float
    domain: str
    related_theories: List[str]

@dataclass
class ConceptualConnection:
    """Represents a connection between research concepts"""
    concept_a: str
    concept_b: str
    connection_type: str  # 'causal', 'correlational', 'analogical', 'contradictory'
    strength: float
    evidence: List[str]
    mechanism: str
    implications: List[str]

@dataclass
class ResearchDirection:
    """Represents a suggested research direction"""
    title: str
    description: str
    methodology: str
    timeline_estimate: str
    resource_requirements: List[str]
    expected_impact: str
    difficulty_level: str
    interdisciplinary_connections: List[str]
    potential_pitfalls: List[str]
    success_metrics: List[str]

class TheoryCraftingService:
    """Service for intelligent research theorycrafting and analysis"""
    
    def __init__(self):
        self.concepts: Dict[str, ResearchConcept] = {}
        self.hypotheses: List[ResearchHypothesis] = []
        self.connections: List[ConceptualConnection] = []
        self.research_directions: List[ResearchDirection] = []
        self.conversation_history: List[Dict[str, Any]] = []
        
        # Initialize ML-powered source recommender
        if ML_RECOMMENDER_AVAILABLE:
            self.ml_recommender = MLSourceRecommender()
        else:
            self.ml_recommender = None
        self.domain_expertise = {
            'space_technology': 0.9,
            'manufacturing': 0.8,
            'automation': 0.9,
            'ai_ml': 0.8,
            'orbital_mechanics': 0.9,
            'systems_engineering': 0.8,
            'data_science': 0.8,
            'business_intelligence': 0.7,
            'innovation_theory': 0.8,
            'complexity_science': 0.7
        }
    
    async def analyze_research_topic(self, topic: str, context: str = "", user_background: str = "") -> Dict[str, Any]:
        """
        Comprehensive analysis of a research topic with theorycrafting
        
        Args:
            topic: The main research topic or question
            context: Additional context or discussion content
            user_background: User's domain expertise or background
        
        Returns:
            Comprehensive analysis with theoretical insights
        """
        logger.info(f"Starting theorycrafting analysis for topic: {topic}")
        
        analysis = {
            'topic': topic,
            'timestamp': datetime.now().isoformat(),
            'theoretical_framework': await self._build_theoretical_framework(topic, context),
            'concept_analysis': await self._analyze_concepts(topic, context),
            'hypotheses': await self._generate_hypotheses(topic, context),
            'research_directions': await self._suggest_research_directions(topic, context),
            'conceptual_connections': await self._find_conceptual_connections(topic, context),
            'knowledge_gaps': await self._identify_knowledge_gaps(topic),
            'methodological_suggestions': await self._suggest_methodologies(topic, context),
            'interdisciplinary_insights': await self._generate_interdisciplinary_insights(topic),
            'critical_evaluation': await self._critical_evaluation(topic, context),
            'evidence_synthesis': await self._synthesize_evidence(topic),
            'future_implications': await self._analyze_future_implications(topic),
            'practical_applications': await self._identify_practical_applications(topic),
            'ml_source_recommendations': await self._get_ml_source_recommendations(topic, context),
            'meta_analysis': {
                'complexity_level': self._assess_complexity(topic, context),
                'research_maturity': await self._assess_research_maturity(topic),
                'domain_relevance': self._calculate_domain_relevance(topic),
                'innovation_potential': self._assess_innovation_potential(topic, context)
            }
        }
        
        # Store in conversation history for learning
        self.conversation_history.append({
            'topic': topic,
            'context': context,
            'analysis': analysis,
            'timestamp': datetime.now()
        })
        
        logger.info(f"Completed theorycrafting analysis with {len(analysis['hypotheses'])} hypotheses and {len(analysis['research_directions'])} research directions")
        
        return analysis
    
    async def _build_theoretical_framework(self, topic: str, context: str) -> Dict[str, Any]:
        """Build a theoretical framework for the research topic"""
        
        # Extract key concepts and theories
        key_concepts = self._extract_key_concepts(topic + " " + context)
        theoretical_foundations = await self._identify_theoretical_foundations(key_concepts)
        
        framework = {
            'core_concepts': key_concepts[:10],  # Top 10 most relevant concepts
            'theoretical_foundations': theoretical_foundations,
            'conceptual_model': self._build_conceptual_model(key_concepts),
            'assumptions': self._identify_assumptions(topic, context),
            'boundary_conditions': self._define_boundary_conditions(topic),
            'variables': self._identify_variables(topic, context),
            'relationships': self._map_relationships(key_concepts)
        }
        
        return framework
    
    async def _analyze_concepts(self, topic: str, context: str) -> List[Dict[str, Any]]:
        """Analyze individual concepts within the research topic"""
        
        concepts = self._extract_key_concepts(topic + " " + context)
        concept_analysis = []
        
        for concept in concepts[:15]:  # Analyze top 15 concepts
            analysis = {
                'concept': concept,
                'definition': self._generate_concept_definition(concept),
                'domain_classification': self._classify_concept_domain(concept),
                'maturity_level': self._assess_concept_maturity(concept),
                'research_volume': await self._estimate_research_volume(concept),
                'related_concepts': self._find_related_concepts(concept, concepts),
                'applications': self._identify_concept_applications(concept),
                'limitations': self._identify_concept_limitations(concept),
                'future_potential': self._assess_concept_future_potential(concept)
            }
            concept_analysis.append(analysis)
        
        return concept_analysis
    
    async def _generate_hypotheses(self, topic: str, context: str) -> List[Dict[str, Any]]:
        """Generate research hypotheses based on topic analysis"""
        
        hypotheses = []
        
        # Generate different types of hypotheses
        causal_hypotheses = self._generate_causal_hypotheses(topic, context)
        correlational_hypotheses = self._generate_correlational_hypotheses(topic, context)
        predictive_hypotheses = self._generate_predictive_hypotheses(topic, context)
        comparative_hypotheses = self._generate_comparative_hypotheses(topic, context)
        
        all_hypotheses = causal_hypotheses + correlational_hypotheses + predictive_hypotheses + comparative_hypotheses
        
        # Rank and select best hypotheses
        ranked_hypotheses = sorted(all_hypotheses, key=lambda h: h.confidence_level, reverse=True)
        
        return [asdict(h) for h in ranked_hypotheses[:8]]  # Top 8 hypotheses
    
    async def _suggest_research_directions(self, topic: str, context: str) -> List[Dict[str, Any]]:
        """Suggest potential research directions"""
        
        directions = []
        
        # Generate different types of research directions
        empirical_directions = self._generate_empirical_directions(topic, context)
        theoretical_directions = self._generate_theoretical_directions(topic, context)
        applied_directions = self._generate_applied_directions(topic, context)
        interdisciplinary_directions = self._generate_interdisciplinary_directions(topic, context)
        
        all_directions = empirical_directions + theoretical_directions + applied_directions + interdisciplinary_directions
        
        # Score and rank directions
        scored_directions = [(d, self._score_research_direction(d)) for d in all_directions]
        ranked_directions = sorted(scored_directions, key=lambda x: x[1], reverse=True)
        
        return [asdict(d[0]) for d in ranked_directions[:6]]  # Top 6 directions
    
    async def _find_conceptual_connections(self, topic: str, context: str) -> List[Dict[str, Any]]:
        """Find and analyze connections between concepts"""
        
        concepts = self._extract_key_concepts(topic + " " + context)
        connections = []
        
        # Find pairwise connections between concepts
        for i, concept_a in enumerate(concepts[:10]):
            for concept_b in concepts[i+1:10]:
                connection = self._analyze_concept_pair(concept_a, concept_b, topic)
                if connection and connection.strength > 0.3:  # Only include significant connections
                    connections.append(connection)
        
        # Sort by connection strength
        connections.sort(key=lambda c: c.strength, reverse=True)
        
        return [asdict(c) for c in connections[:12]]  # Top 12 connections
    
    async def _identify_knowledge_gaps(self, topic: str) -> List[Dict[str, Any]]:
        """Identify gaps in current knowledge"""
        
        gaps = []
        
        # Different types of knowledge gaps
        empirical_gaps = self._identify_empirical_gaps(topic)
        theoretical_gaps = self._identify_theoretical_gaps(topic)
        methodological_gaps = self._identify_methodological_gaps(topic)
        practical_gaps = self._identify_practical_gaps(topic)
        
        all_gaps = empirical_gaps + theoretical_gaps + methodological_gaps + practical_gaps
        
        return all_gaps[:10]  # Top 10 most significant gaps
    
    async def _suggest_methodologies(self, topic: str, context: str) -> List[Dict[str, Any]]:
        """Suggest research methodologies appropriate for the topic"""
        
        methodologies = []
        
        # Determine research type and suggest appropriate methodologies
        research_type = self._classify_research_type(topic, context)
        domain = self._determine_primary_domain(topic)
        
        if research_type == 'empirical':
            methodologies.extend(self._suggest_empirical_methodologies(topic, domain))
        elif research_type == 'theoretical':
            methodologies.extend(self._suggest_theoretical_methodologies(topic, domain))
        elif research_type == 'applied':
            methodologies.extend(self._suggest_applied_methodologies(topic, domain))
        else:
            methodologies.extend(self._suggest_mixed_methodologies(topic, domain))
        
        # Add cross-cutting methodologies
        methodologies.extend(self._suggest_cross_cutting_methodologies(topic))
        
        return methodologies[:8]  # Top 8 methodological suggestions
    
    async def _generate_interdisciplinary_insights(self, topic: str) -> List[Dict[str, Any]]:
        """Generate insights by connecting topic to other disciplines"""
        
        insights = []
        primary_domain = self._determine_primary_domain(topic)
        
        # Connect to other domains
        for domain, expertise in self.domain_expertise.items():
            if domain != primary_domain and expertise > 0.6:
                insight = self._generate_cross_domain_insight(topic, primary_domain, domain)
                if insight:
                    insights.append(insight)
        
        return insights[:6]  # Top 6 interdisciplinary insights
    
    def _extract_key_concepts(self, text: str) -> List[str]:
        """Extract key concepts from text using domain knowledge"""
        
        # Advanced concept extraction combining multiple approaches
        concepts = set()
        
        # Domain-specific terminology
        domain_terms = {
            'space': ['orbital', 'satellite', 'spacecraft', 'mission', 'trajectory', 'propulsion', 'payload'],
            'manufacturing': ['production', 'assembly', 'quality', 'automation', 'process', 'efficiency'],
            'ai': ['machine learning', 'neural network', 'algorithm', 'artificial intelligence', 'data'],
            'systems': ['integration', 'architecture', 'design', 'optimization', 'performance', 'reliability']
        }
        
        text_lower = text.lower()
        
        # Extract domain terms
        for domain, terms in domain_terms.items():
            for term in terms:
                if term in text_lower:
                    concepts.add(term.replace('_', ' ').title())
        
        # Extract multi-word technical concepts
        technical_patterns = [
            r'/b[A-Z][a-z]+/s+[A-Z][a-z]+(?:/s+[A-Z][a-z]+)?/b',  # Multi-word proper nouns
            r'/b/w+ing/s+/w+/b',  # Process descriptions
            r'/b/w+/s+(?:technology|system|method|approach|framework)/b'  # Technical systems
        ]
        
        for pattern in technical_patterns:
            matches = re.findall(pattern, text)
            concepts.update(matches)
        
        # Score and rank concepts by relevance
        concept_scores = {}
        for concept in concepts:
            score = self._calculate_concept_relevance(concept, text)
            if score > 0.3:  # Threshold for relevance
                concept_scores[concept] = score
        
        # Return ranked concepts
        ranked_concepts = sorted(concept_scores.items(), key=lambda x: x[1], reverse=True)
        return [concept for concept, score in ranked_concepts]
    
    def _calculate_concept_relevance(self, concept: str, text: str) -> float:
        """Calculate how relevant a concept is to the text"""
        
        text_lower = text.lower()
        concept_lower = concept.lower()
        
        # Base frequency score
        frequency = text_lower.count(concept_lower)
        frequency_score = min(frequency / 5.0, 1.0)  # Normalize to 0-1
        
        # Position score (concepts mentioned early are often more important)
        first_position = text_lower.find(concept_lower)
        if first_position == -1:
            position_score = 0
        else:
            position_score = 1.0 - (first_position / len(text))
        
        # Domain relevance score
        domain_score = 0.0
        for domain, expertise in self.domain_expertise.items():
            if domain.replace('_', ' ') in concept_lower:
                domain_score = expertise
                break
        
        # Combined relevance score
        relevance = (frequency_score * 0.4 + position_score * 0.3 + domain_score * 0.3)
        return relevance
    
    async def _identify_theoretical_foundations(self, concepts: List[str]) -> List[Dict[str, Any]]:
        """Identify theoretical foundations relevant to the concepts"""
        
        foundations = []
        
        # Map concepts to theoretical frameworks
        theory_mapping = {
            'systems': ['Systems Theory', 'Complexity Theory', 'Network Theory'],
            'innovation': ['Innovation Diffusion Theory', 'Technology Acceptance Model'],
            'automation': ['Automation Theory', 'Human-Machine Interaction Theory'],
            'space': ['Orbital Mechanics Theory', 'Spacecraft Systems Engineering'],
            'manufacturing': ['Lean Manufacturing Theory', 'Six Sigma', 'Theory of Constraints'],
            'learning': ['Constructivism', 'Social Learning Theory', 'Experiential Learning'],
            'optimization': ['Operations Research', 'Control Theory', 'Game Theory']
        }
        
        for concept in concepts[:8]:
            concept_lower = concept.lower()
            for domain, theories in theory_mapping.items():
                if domain in concept_lower or any(term in concept_lower for term in domain.split()):
                    for theory in theories:
                        foundation = {
                            'theory': theory,
                            'relevance_to_concept': concept,
                            'applicability_score': self._calculate_theory_applicability(theory, concept),
                            'key_principles': self._get_theory_principles(theory),
                            'research_applications': self._get_theory_applications(theory),
                            'limitations': self._get_theory_limitations(theory)
                        }
                        foundations.append(foundation)
        
        # Remove duplicates and rank by applicability
        unique_foundations = {}
        for foundation in foundations:
            key = foundation['theory']
            if key not in unique_foundations or foundation['applicability_score'] > unique_foundations[key]['applicability_score']:
                unique_foundations[key] = foundation
        
        ranked_foundations = sorted(unique_foundations.values(), 
                                  key=lambda f: f['applicability_score'], reverse=True)
        
        return ranked_foundations[:8]  # Top 8 theoretical foundations
    
    def _generate_causal_hypotheses(self, topic: str, context: str) -> List[ResearchHypothesis]:
        """Generate causal research hypotheses"""
        
        hypotheses = []
        concepts = self._extract_key_concepts(topic + " " + context)
        
        # Generate causal relationships between concepts
        for i, cause in enumerate(concepts[:5]):
            for effect in concepts[i+1:6]:
                if cause != effect:
                    hypothesis_text = f"Changes in {cause.lower()} causally influence {effect.lower()}"
                    
                    hypothesis = ResearchHypothesis(
                        hypothesis=hypothesis_text,
                        supporting_evidence=[
                            f"Theoretical literature suggests {cause} affects {effect}",
                            f"Empirical observations indicate correlation between {cause} and {effect}",
                            f"Mechanistic models predict {cause} influences {effect}"
                        ],
                        methodology_suggestions=[
                            "Controlled experimental design",
                            "Longitudinal observational study",
                            "Natural experiment analysis",
                            "Instrumental variables approach"
                        ],
                        expected_outcomes=[
                            f"Measurable change in {effect} following {cause} manipulation",
                            f"Dose-response relationship between {cause} and {effect}",
                            f"Time-lagged effects observable"
                        ],
                        risk_factors=[
                            "Confounding variables",
                            "Reverse causation",
                            "Selection bias",
                            "Measurement error"
                        ],
                        confidence_level=self._calculate_hypothesis_confidence(cause, effect, "causal"),
                        testability_score=self._calculate_testability_score(hypothesis_text),
                        domain=self._determine_primary_domain(topic),
                        related_theories=self._identify_related_theories(cause, effect)
                    )
                    
                    hypotheses.append(hypothesis)
        
        return hypotheses[:4]  # Top 4 causal hypotheses
    
    def _calculate_hypothesis_confidence(self, concept_a: str, concept_b: str, relationship_type: str) -> float:
        """Calculate confidence level for a hypothesis"""
        
        # Base confidence depends on relationship type
        base_confidence = {
            'causal': 0.6,
            'correlational': 0.7,
            'predictive': 0.5,
            'comparative': 0.8
        }.get(relationship_type, 0.5)
        
        # Adjust based on concept familiarity and domain expertise
        domain_a = self._classify_concept_domain(concept_a)
        domain_b = self._classify_concept_domain(concept_b)
        
        domain_expertise_a = self.domain_expertise.get(domain_a, 0.5)
        domain_expertise_b = self.domain_expertise.get(domain_b, 0.5)
        
        expertise_factor = (domain_expertise_a + domain_expertise_b) / 2
        
        # Calculate final confidence
        confidence = base_confidence * 0.7 + expertise_factor * 0.3
        return min(max(confidence, 0.1), 0.95)  # Clamp between 0.1 and 0.95
    
    def _calculate_testability_score(self, hypothesis: str) -> float:
        """Calculate how testable a hypothesis is"""
        
        testability_indicators = [
            'measurable', 'quantifiable', 'observable', 'experimental',
            'controlled', 'variable', 'change', 'increase', 'decrease',
            'correlation', 'relationship', 'effect', 'influence'
        ]
        
        hypothesis_lower = hypothesis.lower()
        indicator_count = sum(1 for indicator in testability_indicators if indicator in hypothesis_lower)
        
        # Base testability score
        base_score = min(indicator_count / len(testability_indicators), 0.8)
        
        # Adjust for complexity (simpler hypotheses are often more testable)
        complexity_penalty = len(hypothesis.split()) / 100.0
        testability = max(base_score - complexity_penalty, 0.1)
        
        return testability
    
    def _generate_correlational_hypotheses(self, topic: str, context: str) -> List[ResearchHypothesis]:
        """Generate correlational research hypotheses"""
        hypotheses = []
        concepts = self._extract_key_concepts(topic + " " + context)
        
        for i, concept_a in enumerate(concepts[:4]):
            for concept_b in concepts[i+1:5]:
                if concept_a != concept_b:
                    hypothesis_text = f"{concept_a.title()} and {concept_b.lower()} are positively correlated"
                    
                    hypothesis = ResearchHypothesis(
                        hypothesis=hypothesis_text,
                        supporting_evidence=[
                            f"Literature suggests correlation between {concept_a} and {concept_b}",
                            f"Theoretical models predict relationship",
                            f"Empirical patterns observed"
                        ],
                        methodology_suggestions=[
                            "Cross-sectional survey",
                            "Correlation analysis",
                            "Regression modeling",
                            "Meta-analysis"
                        ],
                        expected_outcomes=[
                            f"Positive correlation coefficient",
                            f"Statistical significance",
                            f"Effect size estimation"
                        ],
                        risk_factors=[
                            "Spurious correlation",
                            "Third variable problem",
                            "Sampling bias"
                        ],
                        confidence_level=self._calculate_hypothesis_confidence(concept_a, concept_b, "correlational"),
                        testability_score=self._calculate_testability_score(hypothesis_text),
                        domain=self._determine_primary_domain(topic),
                        related_theories=self._identify_related_theories(concept_a, concept_b)
                    )
                    hypotheses.append(hypothesis)
        
        return hypotheses[:3]
    
    def _generate_predictive_hypotheses(self, topic: str, context: str) -> List[ResearchHypothesis]:
        """Generate predictive research hypotheses"""
        hypotheses = []
        concepts = self._extract_key_concepts(topic + " " + context)
        
        for i, predictor in enumerate(concepts[:3]):
            for outcome in concepts[i+1:4]:
                hypothesis_text = f"{predictor.title()} can predict future {outcome.lower()} performance"
                
                hypothesis = ResearchHypothesis(
                    hypothesis=hypothesis_text,
                    supporting_evidence=[
                        f"Historical data shows {predictor} patterns",
                        f"Theoretical basis for prediction",
                        f"Similar models exist in literature"
                    ],
                    methodology_suggestions=[
                        "Time series analysis",
                        "Machine learning models",
                        "Longitudinal study",
                        "Predictive modeling"
                    ],
                    expected_outcomes=[
                        f"Predictive accuracy above baseline",
                        f"Model validation metrics",
                        f"Forecasting capability"
                    ],
                    risk_factors=[
                        "Overfitting",
                        "External validity",
                        "Concept drift"
                    ],
                    confidence_level=self._calculate_hypothesis_confidence(predictor, outcome, "predictive"),
                    testability_score=self._calculate_testability_score(hypothesis_text),
                    domain=self._determine_primary_domain(topic),
                    related_theories=self._identify_related_theories(predictor, outcome)
                )
                hypotheses.append(hypothesis)
        
        return hypotheses[:2]
    
    def _generate_comparative_hypotheses(self, topic: str, context: str) -> List[ResearchHypothesis]:
        """Generate comparative research hypotheses"""
        hypotheses = []
        concepts = self._extract_key_concepts(topic + " " + context)
        
        for i, concept_a in enumerate(concepts[:3]):
            for concept_b in concepts[i+1:4]:
                hypothesis_text = f"{concept_a.title()} is more effective than {concept_b.lower()} for achieving desired outcomes"
                
                hypothesis = ResearchHypothesis(
                    hypothesis=hypothesis_text,
                    supporting_evidence=[
                        f"Theoretical advantages of {concept_a}",
                        f"Comparative case studies",
                        f"Performance metrics comparison"
                    ],
                    methodology_suggestions=[
                        "Randomized controlled trial",
                        "Comparative case study",
                        "A/B testing",
                        "Benchmarking study"
                    ],
                    expected_outcomes=[
                        f"Statistically significant difference",
                        f"Effect size quantification",
                        f"Practical significance"
                    ],
                    risk_factors=[
                        "Selection bias",
                        "Implementation differences",
                        "Context sensitivity"
                    ],
                    confidence_level=self._calculate_hypothesis_confidence(concept_a, concept_b, "comparative"),
                    testability_score=self._calculate_testability_score(hypothesis_text),
                    domain=self._determine_primary_domain(topic),
                    related_theories=self._identify_related_theories(concept_a, concept_b)
                )
                hypotheses.append(hypothesis)
        
        return hypotheses[:2]
    
    def _generate_concept_definition(self, concept: str) -> str:
        """Generate a definition for a research concept"""
        concept_definitions = {
            'automation': 'The use of technology to perform tasks with minimal human intervention',
            'orbital': 'Relating to the curved path of an object around a celestial body',
            'manufacturing': 'The production of goods through industrial processes',
            'optimization': 'The process of making something as effective or functional as possible',
            'integration': 'The coordination of separate elements into a unified whole',
            'innovation': 'The introduction of new ideas, methods, or products',
            'efficiency': 'The ability to accomplish a task with minimal waste of resources',
            'system': 'A set of connected parts forming a complex whole'
        }
        
        return concept_definitions.get(concept.lower(), 
                                     f"A research concept in the domain of {self._classify_concept_domain(concept)}")
    
    def _classify_concept_domain(self, concept: str) -> str:
        """Classify a concept into a research domain"""
        concept_lower = concept.lower()
        
        domain_keywords = {
            'space_technology': ['orbital', 'satellite', 'spacecraft', 'mission', 'trajectory'],
            'manufacturing': ['production', 'assembly', 'quality', 'factory', 'process'],
            'automation': ['robot', 'automatic', 'control', 'smart', 'automated'],
            'ai_ml': ['learning', 'neural', 'algorithm', 'intelligence', 'data'],
            'systems_engineering': ['system', 'integration', 'architecture', 'design'],
            'data_science': ['data', 'analytics', 'statistics', 'analysis'],
            'innovation_theory': ['innovation', 'creativity', 'invention', 'novel']
        }
        
        for domain, keywords in domain_keywords.items():
            if any(keyword in concept_lower for keyword in keywords):
                return domain
        
        return 'general'
    
    def _assess_concept_maturity(self, concept: str) -> str:
        """Assess the maturity level of a research concept"""
        mature_concepts = ['manufacturing', 'automation', 'system', 'data', 'analysis']
        emerging_concepts = ['ai', 'machine learning', 'neural', 'smart', 'autonomous']
        
        concept_lower = concept.lower()
        
        if any(mature in concept_lower for mature in mature_concepts):
            return 'mature'
        elif any(emerging in concept_lower for emerging in emerging_concepts):
            return 'emerging'
        else:
            return 'developing'
    
    def _assess_concept_future_potential(self, concept: str) -> str:
        """Assess the future potential of a research concept"""
        high_potential = ['ai', 'automation', 'space', 'quantum', 'neural', 'smart']
        medium_potential = ['optimization', 'integration', 'efficiency', 'innovation']
        
        concept_lower = concept.lower()
        
        if any(high in concept_lower for high in high_potential):
            return 'high'
        elif any(medium in concept_lower for medium in medium_potential):
            return 'medium'
        else:
            return 'moderate'
    
    def _identify_concept_applications(self, concept: str) -> List[str]:
        """Identify practical applications of a concept"""
        application_mapping = {
            'automation': ['Industrial processes', 'Home systems', 'Transportation', 'Healthcare'],
            'optimization': ['Resource allocation', 'Process improvement', 'Cost reduction', 'Performance enhancement'],
            'integration': ['System connectivity', 'Data fusion', 'Process coordination', 'Technology convergence'],
            'manufacturing': ['Product creation', 'Quality control', 'Supply chain', 'Assembly automation'],
            'orbital': ['Satellite deployment', 'Space missions', 'Communication systems', 'Earth observation']
        }
        
        concept_lower = concept.lower()
        for key_concept, applications in application_mapping.items():
            if key_concept in concept_lower:
                return applications
        
        return ['Research applications', 'Commercial uses', 'Educational purposes', 'Technology development']
    
    def _identify_concept_limitations(self, concept: str) -> List[str]:
        """Identify limitations of a concept"""
        return [
            'Technical complexity',
            'Cost considerations',
            'Implementation challenges',
            'Scalability issues',
            'Regulatory constraints'
        ]
    
    def _determine_primary_domain(self, topic: str) -> str:
        """Determine the primary research domain for a topic"""
        domain_scores = {}
        topic_lower = topic.lower()
        
        for domain in self.domain_expertise.keys():
            domain_clean = domain.replace('_', ' ')
            if domain_clean in topic_lower:
                domain_scores[domain] = self.domain_expertise[domain]
        
        if domain_scores:
            return max(domain_scores.items(), key=lambda x: x[1])[0]
        else:
            return 'general'
    
    def _identify_related_theories(self, concept_a: str, concept_b: str) -> List[str]:
        """Identify theories related to concepts"""
        theories = [
            'Systems Theory',
            'Innovation Theory',
            'Optimization Theory',
            'Complexity Theory',
            'Information Theory'
        ]
        return theories[:3]  # Return top 3 relevant theories
    
    async def _estimate_research_volume(self, concept: str) -> str:
        """Estimate research volume for a concept"""
        high_volume_concepts = ['ai', 'machine learning', 'automation', 'data']
        medium_volume_concepts = ['optimization', 'integration', 'manufacturing']
        
        concept_lower = concept.lower()
        
        if any(high in concept_lower for high in high_volume_concepts):
            return 'high'
        elif any(medium in concept_lower for medium in medium_volume_concepts):
            return 'medium'
        else:
            return 'low'
    
    def _find_related_concepts(self, concept: str, all_concepts: List[str]) -> List[str]:
        """Find concepts related to the given concept"""
        related = []
        concept_lower = concept.lower()
        
        for other_concept in all_concepts:
            if other_concept != concept:
                # Simple relatedness based on domain similarity
                if self._classify_concept_domain(concept) == self._classify_concept_domain(other_concept):
                    related.append(other_concept)
        
        return related[:5]  # Top 5 related concepts
    
    def _analyze_concept_pair(self, concept_a: str, concept_b: str, context: str) -> Optional[ConceptualConnection]:
        """Analyze the connection between two concepts"""
        
        # Determine connection type and strength
        domain_a = self._classify_concept_domain(concept_a)
        domain_b = self._classify_concept_domain(concept_b)
        
        if domain_a == domain_b:
            connection_type = 'correlational'
            strength = 0.7
        elif self._are_causally_related(concept_a, concept_b):
            connection_type = 'causal'
            strength = 0.8
        else:
            connection_type = 'analogical'
            strength = 0.4
        
        if strength > 0.3:
            return ConceptualConnection(
                concept_a=concept_a,
                concept_b=concept_b,
                connection_type=connection_type,
                strength=strength,
                evidence=[f"Domain analysis suggests {connection_type} relationship"],
                mechanism=f"Connection through {domain_a} principles",
                implications=[f"Research in {concept_a} may inform {concept_b}"]
            )
        return None
    
    def _are_causally_related(self, concept_a: str, concept_b: str) -> bool:
        """Check if two concepts might be causally related"""
        causal_patterns = [
            ('automation', 'efficiency'),
            ('optimization', 'performance'),
            ('integration', 'coordination'),
            ('innovation', 'advancement')
        ]
        
        concept_a_lower = concept_a.lower()
        concept_b_lower = concept_b.lower()
        
        for cause, effect in causal_patterns:
            if cause in concept_a_lower and effect in concept_b_lower:
                return True
        
        return False
    
    # Add placeholder methods for missing functionality
    def _build_conceptual_model(self, concepts: List[str]) -> Dict[str, Any]:
        """Build a conceptual model from key concepts"""
        return {
            'core_concepts': concepts[:5],
            'relationships': ['hierarchical', 'causal', 'correlational'],
            'model_type': 'systems_based'
        }
    
    def _identify_assumptions(self, topic: str, context: str) -> List[str]:
        """Identify key assumptions underlying the research topic"""
        return [
            'Variables are measurable',
            'Relationships are stable over time',
            'Context effects are minimal',
            'Sample is representative'
        ]
    
    def _define_boundary_conditions(self, topic: str) -> List[str]:
        """Define boundary conditions for the research"""
        return [
            'Scope limitations',
            'Time constraints',
            'Resource availability',
            'Technical feasibility'
        ]
    
    def _identify_variables(self, topic: str, context: str) -> Dict[str, List[str]]:
        """Identify research variables"""
        return {
            'independent': ['Primary factor', 'Control variable'],
            'dependent': ['Outcome measure', 'Performance metric'],
            'moderating': ['Context factor', 'Environmental variable'],
            'mediating': ['Process variable', 'Mechanism']
        }
    
    def _map_relationships(self, concepts: List[str]) -> List[Dict[str, str]]:
        """Map relationships between concepts"""
        relationships = []
        for i, concept_a in enumerate(concepts[:3]):
            for concept_b in concepts[i+1:4]:
                relationships.append({
                    'from': concept_a,
                    'to': concept_b,
                    'type': 'influences'
                })
        return relationships
    
    def _calculate_theory_applicability(self, theory: str, concept: str) -> float:
        """Calculate how applicable a theory is to a concept"""
        return 0.7  # Default applicability score
    
    def _get_theory_principles(self, theory: str) -> List[str]:
        """Get key principles of a theory"""
        return [
            'Principle 1: Core assumption',
            'Principle 2: Key mechanism',
            'Principle 3: Boundary condition'
        ]
    
    def _get_theory_applications(self, theory: str) -> List[str]:
        """Get research applications of a theory"""
        return [
            'Empirical testing',
            'Model development',
            'Practical implementation'
        ]
    
    def _get_theory_limitations(self, theory: str) -> List[str]:
        """Get limitations of a theory"""
        return [
            'Scope restrictions',
            'Empirical gaps',
            'Methodological challenges'
        ]
    
    async def _synthesize_evidence(self, topic: str) -> Dict[str, Any]:
        """Synthesize evidence from scraped articles, reports, and other sources"""
        
        if not DATABASE_AVAILABLE:
            logger.warning("Database not available for evidence synthesis")
            return {
                'evidence_sources': [],
                'synthesis_summary': 'No database access for evidence synthesis',
                'confidence_level': 0.3
            }
        
        try:
            evidence_synthesis = {
                'scraped_articles': [],
                'generated_reports': [],
                'entity_insights': [],
                'synthesis_summary': '',
                'confidence_level': 0.0,
                'evidence_strength': 'weak',
                'supporting_sources': [],
                'contradicting_sources': [],
                'research_gaps_identified': []
            }
            
            # Search for relevant articles from scraping
            async for session in get_postgres_session():
                # Find articles related to the topic
                article_query = select(Article).where(
                    or_(
                        Article.title.ilike(f'%{topic}%'),
                        Article.content.ilike(f'%{topic}%'),
                        Article.summary.ilike(f'%{topic}%')
                    )
                ).limit(20)
                
                articles_result = await session.execute(article_query)
                relevant_articles = articles_result.scalars().all()
                
                # Extract evidence from articles
                for article in relevant_articles:
                    evidence_synthesis['scraped_articles'].append({
                        'title': article.title,
                        'source_url': article.url,
                        'relevance_score': article.relevance_score or 0.5,
                        'quality_score': article.quality_score or 0.5,
                        'sentiment_score': article.sentiment_score or 0.0,
                        'key_insights': self._extract_insights_from_article(article, topic),
                        'publication_date': article.published_date.isoformat() if article.published_date else None,
                        'word_count': article.word_count or 0
                    })
                
                # Find relevant reports
                report_query = select(Report).where(
                    or_(
                        Report.title.ilike(f'%{topic}%'),
                        Report.full_content.ilike(f'%{topic}%'),
                        Report.executive_summary.ilike(f'%{topic}%')
                    )
                ).limit(10)
                
                reports_result = await session.execute(report_query)
                relevant_reports = reports_result.scalars().all()
                
                # Extract insights from reports
                for report in relevant_reports:
                    evidence_synthesis['generated_reports'].append({
                        'title': report.title,
                        'report_type': report.report_type,
                        'confidence_score': report.confidence_score or 0.5,
                        'key_findings': report.key_findings or [],
                        'recommendations': report.recommendations or [],
                        'risks': report.risks or [],
                        'opportunities': report.opportunities or [],
                        'generation_date': report.created_at.isoformat(),
                        'word_count': report.word_count or 0
                    })
                
                # Find relevant entities
                entity_query = select(Entity).where(
                    or_(
                        Entity.name.ilike(f'%{topic}%'),
                        Entity.description.ilike(f'%{topic}%')
                    )
                ).limit(15)
                
                entities_result = await session.execute(entity_query)
                relevant_entities = entities_result.scalars().all()
                
                # Extract entity insights
                for entity in relevant_entities:
                    evidence_synthesis['entity_insights'].append({
                        'name': entity.name,
                        'entity_type': entity.entity_type,
                        'description': entity.description,
                        'is_space_related': entity.is_space_related,
                        'space_sector': entity.space_sector,
                        'mention_count': entity.mention_count or 0,
                        'sentiment_average': entity.sentiment_average or 0.0,
                        'last_mentioned': entity.last_mentioned.isoformat() if entity.last_mentioned else None
                    })
                
                break  # Exit the async generator after first session
            
            # Calculate synthesis metrics
            total_sources = len(evidence_synthesis['scraped_articles']) + len(evidence_synthesis['generated_reports'])
            if total_sources > 0:
                avg_quality = sum(
                    [article['quality_score'] for article in evidence_synthesis['scraped_articles']] +
                    [report['confidence_score'] for report in evidence_synthesis['generated_reports']]
                ) / total_sources
                
                evidence_synthesis['confidence_level'] = avg_quality
                evidence_synthesis['evidence_strength'] = self._classify_evidence_strength(total_sources, avg_quality)
            
            # Generate synthesis summary
            evidence_synthesis['synthesis_summary'] = self._generate_evidence_summary(evidence_synthesis, topic)
            
            # Identify supporting vs contradicting sources
            evidence_synthesis['supporting_sources'] = self._identify_supporting_sources(evidence_synthesis, topic)
            evidence_synthesis['contradicting_sources'] = self._identify_contradicting_sources(evidence_synthesis, topic)
            
            # Identify research gaps
            evidence_synthesis['research_gaps_identified'] = self._identify_evidence_gaps(evidence_synthesis, topic)
            
            logger.info(f"Evidence synthesis completed: {total_sources} sources analyzed")
            return evidence_synthesis
            
        except Exception as e:
            logger.error(f"Error in evidence synthesis: {e}")
            return {
                'error': str(e),
                'evidence_sources': [],
                'synthesis_summary': 'Evidence synthesis failed due to database error',
                'confidence_level': 0.1
            }
    
    def _extract_insights_from_article(self, article: Any, topic: str) -> List[str]:
        """Extract key insights from an article relevant to the topic"""
        insights = []
        
        # Extract insights from summary if available
        if article.summary:
            summary_sentences = article.summary.split('.')
            for sentence in summary_sentences[:3]:  # Top 3 sentences
                if topic.lower() in sentence.lower() and len(sentence.strip()) > 20:
                    insights.append(sentence.strip())
        
        # Extract from key findings if available
        if hasattr(article, 'extracted_facts') and article.extracted_facts:
            facts = article.extracted_facts[:3]  # Top 3 facts
            insights.extend([fact for fact in facts if isinstance(fact, str)])
        
        # If no insights found, create generic ones
        if not insights:
            insights = [
                f"Article discusses {topic} in context of {article.title}",
                f"Relevance score: {article.relevance_score or 0.5}",
                f"Quality assessment: {article.quality_score or 0.5}"
            ]
        
        return insights[:5]  # Limit to 5 insights per article
    
    def _classify_evidence_strength(self, source_count: int, avg_quality: float) -> str:
        """Classify the strength of evidence based on source count and quality"""
        if source_count >= 10 and avg_quality >= 0.7:
            return 'strong'
        elif source_count >= 5 and avg_quality >= 0.6:
            return 'moderate'
        elif source_count >= 2 and avg_quality >= 0.5:
            return 'weak'
        else:
            return 'insufficient'
    
    def _generate_evidence_summary(self, evidence: Dict[str, Any], topic: str) -> str:
        """Generate a summary of the evidence synthesis"""
        article_count = len(evidence['scraped_articles'])
        report_count = len(evidence['generated_reports'])
        entity_count = len(evidence['entity_insights'])
        
        summary = f"Evidence synthesis for '{topic}' identified {article_count} relevant articles, "
        summary += f"{report_count} generated reports, and {entity_count} related entities. "
        
        if evidence['confidence_level'] > 0.7:
            summary += "High confidence in evidence quality with strong theoretical support. "
        elif evidence['confidence_level'] > 0.5:
            summary += "Moderate confidence in evidence with reasonable theoretical support. "
        else:
            summary += "Limited evidence available requiring additional research. "
        
        summary += f"Evidence strength assessed as {evidence['evidence_strength']}."
        
        return summary
    
    def _identify_supporting_sources(self, evidence: Dict[str, Any], topic: str) -> List[str]:
        """Identify sources that support the research topic"""
        supporting = []
        
        # Check articles with positive sentiment and high relevance
        for article in evidence['scraped_articles']:
            if article['sentiment_score'] > 0.2 and article['relevance_score'] > 0.6:
                supporting.append(f"Article: {article['title']} (Quality: {article['quality_score']:.2f})")
        
        # Check reports with positive findings
        for report in evidence['generated_reports']:
            if report['confidence_score'] > 0.6 and len(report.get('opportunities', [])) > 0:
                supporting.append(f"Report: {report['title']} (Confidence: {report['confidence_score']:.2f})")
        
        return supporting[:8]  # Limit to top 8 supporting sources
    
    def _identify_contradicting_sources(self, evidence: Dict[str, Any], topic: str) -> List[str]:
        """Identify sources that contradict or present challenges to the research topic"""
        contradicting = []
        
        # Check articles with negative sentiment
        for article in evidence['scraped_articles']:
            if article['sentiment_score'] < -0.2 and article['relevance_score'] > 0.5:
                contradicting.append(f"Article: {article['title']} (Negative sentiment: {article['sentiment_score']:.2f})")
        
        # Check reports with significant risks
        for report in evidence['generated_reports']:
            if len(report.get('risks', [])) > 2:
                contradicting.append(f"Report: {report['title']} (High risk factors identified)")
        
        return contradicting[:5]  # Limit to top 5 contradicting sources
    
    def _identify_evidence_gaps(self, evidence: Dict[str, Any], topic: str) -> List[str]:
        """Identify gaps in the available evidence"""
        gaps = []
        
        article_count = len(evidence['scraped_articles'])
        report_count = len(evidence['generated_reports'])
        
        if article_count < 5:
            gaps.append("Limited primary source articles - need more empirical data")
        
        if report_count < 2:
            gaps.append("Few analytical reports available - need more synthesis studies")
        
        if evidence['confidence_level'] < 0.6:
            gaps.append("Low overall evidence quality - need higher quality sources")
        
        # Check for temporal gaps
        recent_sources = sum(1 for article in evidence['scraped_articles'] 
                           if article.get('publication_date') and 
                           article['publication_date'] > '2023-01-01')
        
        if recent_sources < 3:
            gaps.append("Limited recent research - need more current studies")
        
        # Check for methodological gaps
        methodological_diversity = len(set(article.get('source_url', '').split('.')[1] 
                                         for article in evidence['scraped_articles'] 
                                         if article.get('source_url')))
        
        if methodological_diversity < 3:
            gaps.append("Limited source diversity - need varied research approaches")
        
        return gaps[:6]  # Limit to top 6 gaps
    
    # Add remaining missing methods that are referenced in the main analysis
    async def _identify_knowledge_gaps(self, topic: str) -> List[Dict[str, Any]]:
        """Identify knowledge gaps by analyzing available evidence"""
        evidence = await self._synthesize_evidence(topic)
        
        gaps = []
        for gap_description in evidence.get('research_gaps_identified', []):
            gaps.append({
                'gap_type': 'empirical',
                'description': gap_description,
                'priority': 'high',
                'potential_impact': 'significant',
                'research_approach': 'systematic review'
            })
        
        # Add theoretical gaps
        concepts = self._extract_key_concepts(topic)
        for concept in concepts[:3]:
            gaps.append({
                'gap_type': 'theoretical',
                'description': f"Limited theoretical framework for {concept}",
                'priority': 'medium',
                'potential_impact': 'moderate',
                'research_approach': 'theoretical modeling'
            })
        
        return gaps[:8]
    
    async def _suggest_methodologies(self, topic: str, context: str) -> List[Dict[str, Any]]:
        """Suggest research methodologies with evidence-based recommendations"""
        methodologies = []
        
        # Base methodologies on domain and research type
        domain = self._determine_primary_domain(topic)
        research_type = self._classify_research_type(topic, context)
        
        if domain == 'space_technology':
            methodologies.extend([
                {
                    'methodology': 'Simulation Modeling',
                    'description': 'Computational models for space systems analysis',
                    'applicability_score': 0.9,
                    'resource_requirements': ['Computational resources', 'Domain expertise'],
                    'timeline_estimate': '6-12 months',
                    'evidence_support': 'Strong support from space industry literature'
                },
                {
                    'methodology': 'Case Study Analysis',
                    'description': 'Detailed analysis of space missions and projects',
                    'applicability_score': 0.8,
                    'resource_requirements': ['Historical data', 'Domain expertise'],
                    'timeline_estimate': '3-6 months',
                    'evidence_support': 'Moderate support from mission archives'
                }
            ])
        
        if domain == 'manufacturing':
            methodologies.extend([
                {
                    'methodology': 'Process Analysis',
                    'description': 'Systematic analysis of manufacturing processes',
                    'applicability_score': 0.8,
                    'resource_requirements': ['Access to manufacturing facilities', 'Process data'],
                    'timeline_estimate': '4-8 months',
                    'evidence_support': 'Strong support from industrial engineering literature'
                }
            ])
        
        # Add evidence-informed methodologies based on scraped sources
        evidence = await self._synthesize_evidence(topic)
        if evidence['evidence_strength'] == 'strong':
            methodologies.append({
                'methodology': 'Meta-Analysis',
                'description': 'Systematic analysis of existing research findings',
                'applicability_score': 0.9,
                'resource_requirements': ['Statistical software', 'Research database access'],
                'timeline_estimate': '4-6 months',
                'evidence_support': f"Supported by {len(evidence['scraped_articles'])} available studies"
            })
        
        return methodologies[:6]
    
    async def _generate_interdisciplinary_insights(self, topic: str) -> List[Dict[str, Any]]:
        """Generate interdisciplinary insights using scraped data"""
        insights = []
        primary_domain = self._determine_primary_domain(topic)
        
        # Get evidence to inform cross-domain connections
        evidence = await self._synthesize_evidence(topic)
        
        # Analyze entity connections for interdisciplinary opportunities
        space_entities = [e for e in evidence.get('entity_insights', []) if e.get('is_space_related')]
        other_entities = [e for e in evidence.get('entity_insights', []) if not e.get('is_space_related')]
        
        if space_entities and other_entities:
            insights.append({
                'insight_type': 'cross_sector_application',
                'description': f"Space technology applications in {primary_domain}",
                'connection_strength': 0.7,
                'evidence_base': f"{len(space_entities)} space entities, {len(other_entities)} other entities",
                'potential_impact': 'high',
                'research_opportunities': [
                    'Technology transfer studies',
                    'Comparative analysis',
                    'Innovation pathway mapping'
                ]
            })
        
        # Add domain-crossing insights
        for domain, expertise in self.domain_expertise.items():
            if domain != primary_domain and expertise > 0.6:
                insights.append({
                    'insight_type': 'domain_integration',
                    'description': f"Integration of {primary_domain} with {domain.replace('_', ' ')}",
                    'connection_strength': expertise,
                    'evidence_base': 'Domain expertise analysis',
                    'potential_impact': 'moderate',
                    'research_opportunities': [
                        'Conceptual framework development',
                        'Empirical validation studies',
                        'Application development'
                    ]
                })
        
        return insights[:5]
    
    async def _critical_evaluation(self, topic: str, context: str) -> Dict[str, Any]:
        """Perform critical evaluation using evidence from scraped sources"""
        evidence = await self._synthesize_evidence(topic)
        
        evaluation = {
            'strengths': [],
            'weaknesses': [],
            'threats': [],
            'opportunities': [],
            'overall_assessment': '',
            'confidence_level': evidence.get('confidence_level', 0.5)
        }
        
        # Analyze strengths from supporting sources
        if len(evidence.get('supporting_sources', [])) > 3:
            evaluation['strengths'].append('Strong empirical support from multiple sources')
        
        if evidence.get('evidence_strength') == 'strong':
            evaluation['strengths'].append('High quality evidence base available')
        
        # Analyze weaknesses from contradicting sources and gaps
        if len(evidence.get('contradicting_sources', [])) > 1:
            evaluation['weaknesses'].append('Some contradictory evidence exists')
        
        for gap in evidence.get('research_gaps_identified', [])[:2]:
            evaluation['weaknesses'].append(f"Gap identified: {gap}")
        
        # Opportunities from entity insights
        space_related_entities = sum(1 for e in evidence.get('entity_insights', []) if e.get('is_space_related'))
        if space_related_entities > 2:
            evaluation['opportunities'].append('Multiple space industry applications possible')
        
        # Threats from risk analysis
        high_risk_reports = [r for r in evidence.get('generated_reports', []) if len(r.get('risks', [])) > 2]
        if high_risk_reports:
            evaluation['threats'].append('Significant risk factors identified in reports')
        
        # Overall assessment
        if evaluation['confidence_level'] > 0.7:
            evaluation['overall_assessment'] = 'Strong theoretical and empirical foundation with clear research potential'
        elif evaluation['confidence_level'] > 0.5:
            evaluation['overall_assessment'] = 'Moderate foundation with some research opportunities'
        else:
            evaluation['overall_assessment'] = 'Limited evidence base requiring foundational research'
        
        return evaluation
    
    async def _analyze_future_implications(self, topic: str) -> Dict[str, Any]:
        """Analyze future implications based on current evidence and trends"""
        evidence = await self._synthesize_evidence(topic)
        
        implications = {
            'short_term': [],  # 1-2 years
            'medium_term': [],  # 3-5 years
            'long_term': [],  # 5+ years
            'uncertainty_factors': [],
            'scenario_planning': {
                'best_case': '',
                'most_likely': '',
                'worst_case': ''
            }
        }
        
        # Short-term implications from recent articles
        recent_articles = [a for a in evidence.get('scraped_articles', []) 
                          if a.get('publication_date', '2020') > '2023-01-01']
        
        if recent_articles:
            implications['short_term'].append('Immediate research applications from current studies')
            implications['short_term'].append('Technology readiness improvements expected')
        
        # Medium-term based on report recommendations
        report_opportunities = []
        for report in evidence.get('generated_reports', []):
            report_opportunities.extend(report.get('opportunities', [])[:2])
        
        if report_opportunities:
            implications['medium_term'].extend(report_opportunities[:3])
        
        # Long-term implications from domain trends
        domain = self._determine_primary_domain(topic)
        if domain == 'space_technology':
            implications['long_term'].extend([
                'Commercial space industry expansion',
                'Space manufacturing capabilities development',
                'Interplanetary mission applications'
            ])
        elif domain == 'automation':
            implications['long_term'].extend([
                'Full process automation implementation',
                'Human-AI collaboration evolution',
                'Industry transformation'
            ])
        
        # Uncertainty factors from risks
        risk_factors = []
        for report in evidence.get('generated_reports', []):
            risk_factors.extend(report.get('risks', [])[:2])
        
        implications['uncertainty_factors'] = list(set(risk_factors))[:4]
        
        # Scenario planning
        if evidence.get('evidence_strength') == 'strong':
            implications['scenario_planning']['best_case'] = 'Rapid advancement with strong theoretical support and practical applications'
            implications['scenario_planning']['most_likely'] = 'Steady progress with incremental improvements and applications'
            implications['scenario_planning']['worst_case'] = 'Some challenges but continued research interest'
        else:
            implications['scenario_planning']['best_case'] = 'Foundational research leads to breakthrough insights'
            implications['scenario_planning']['most_likely'] = 'Gradual knowledge building with limited practical impact'
            implications['scenario_planning']['worst_case'] = 'Research interest declines due to limited evidence'
        
        return implications
    
    async def _identify_practical_applications(self, topic: str) -> List[Dict[str, Any]]:
        """Identify practical applications based on evidence and entity analysis"""
        evidence = await self._synthesize_evidence(topic)
        applications = []
        
        # Applications from entity analysis
        for entity in evidence.get('entity_insights', [])[:5]:
            if entity.get('is_space_related') and entity.get('mention_count', 0) > 2:
                applications.append({
                    'application_area': 'Space Industry',
                    'specific_application': f"{entity['name']} integration",
                    'readiness_level': 'developing',
                    'market_potential': 'high',
                    'evidence_support': f"Mentioned {entity['mention_count']} times in sources",
                    'implementation_timeline': '2-3 years'
                })
        
        # Applications from report opportunities
        for report in evidence.get('generated_reports', []):
            for opportunity in report.get('opportunities', [])[:2]:
                applications.append({
                    'application_area': 'Research Application',
                    'specific_application': opportunity,
                    'readiness_level': 'conceptual',
                    'market_potential': 'moderate',
                    'evidence_support': f"Identified in {report['title']}",
                    'implementation_timeline': '3-5 years'
                })
        
        # Domain-specific applications
        domain = self._determine_primary_domain(topic)
        concepts = self._extract_key_concepts(topic)
        
        for concept in concepts[:3]:
            concept_applications = self._identify_concept_applications(concept)
            for app in concept_applications[:2]:
                applications.append({
                    'application_area': domain.replace('_', ' ').title(),
                    'specific_application': app,
                    'readiness_level': 'mature' if domain in ['manufacturing', 'automation'] else 'developing',
                    'market_potential': 'moderate',
                    'evidence_support': 'Domain analysis',
                    'implementation_timeline': '1-2 years'
                })
        
        return applications[:8]  # Limit to 8 applications
    
    # Helper methods for classification
    def _classify_research_type(self, topic: str, context: str) -> str:
        """Classify the type of research (empirical, theoretical, applied, mixed)"""
        text = (topic + " " + context).lower()
        
        empirical_indicators = ['data', 'experiment', 'study', 'analysis', 'measurement', 'observation']
        theoretical_indicators = ['model', 'theory', 'framework', 'concept', 'principle']
        applied_indicators = ['application', 'implementation', 'practice', 'solution', 'system']
        
        empirical_score = sum(1 for indicator in empirical_indicators if indicator in text)
        theoretical_score = sum(1 for indicator in theoretical_indicators if indicator in text)
        applied_score = sum(1 for indicator in applied_indicators if indicator in text)
        
        if empirical_score > theoretical_score and empirical_score > applied_score:
            return 'empirical'
        elif theoretical_score > applied_score:
            return 'theoretical'
        elif applied_score > 0:
            return 'applied'
        else:
            return 'mixed'
    
    def _assess_complexity(self, topic: str, context: str) -> str:
        """Assess the complexity level of the research topic"""
        concepts = self._extract_key_concepts(topic + " " + context)
        concept_count = len(concepts)
        
        if concept_count > 10:
            return 'high'
        elif concept_count > 5:
            return 'medium'
        else:
            return 'low'
    
    async def _assess_research_maturity(self, topic: str) -> str:
        """Assess research maturity based on available evidence"""
        evidence = await self._synthesize_evidence(topic)
        
        article_count = len(evidence.get('scraped_articles', []))
        report_count = len(evidence.get('generated_reports', []))
        
        if article_count > 15 and report_count > 3:
            return 'mature'
        elif article_count > 5 and report_count > 1:
            return 'developing'
        else:
            return 'emerging'
    
    def _calculate_domain_relevance(self, topic: str) -> Dict[str, float]:
        """Calculate relevance scores for different domains"""
        relevance_scores = {}
        topic_lower = topic.lower()
        
        for domain, expertise in self.domain_expertise.items():
            domain_terms = domain.replace('_', ' ').split()
            relevance = 0.0
            
            for term in domain_terms:
                if term in topic_lower:
                    relevance += 0.3
            
            # Boost relevance based on expertise level
            relevance = min(relevance * expertise, 1.0)
            relevance_scores[domain] = relevance
        
        return relevance_scores
    
    def _assess_innovation_potential(self, topic: str, context: str) -> float:
        """Assess innovation potential of the research topic"""
        innovative_terms = ['novel', 'new', 'innovative', 'breakthrough', 'cutting-edge', 'advanced']
        text = (topic + " " + context).lower()
        
        innovation_score = sum(0.2 for term in innovative_terms if term in text)
        
        # Boost score for emerging domains
        domain = self._determine_primary_domain(topic)
        if domain in ['ai_ml', 'space_technology']:
            innovation_score += 0.3
        
        return min(innovation_score, 1.0)
    
    # Add missing research direction generation methods
    def _generate_empirical_directions(self, topic: str, context: str) -> List[ResearchDirection]:
        """Generate empirical research directions"""
        directions = []
        concepts = self._extract_key_concepts(topic + " " + context)
        
        for concept in concepts[:3]:
            direction = ResearchDirection(
                title=f"Empirical Study of {concept} in {self._determine_primary_domain(topic)}",
                description=f"Systematic empirical investigation of {concept} mechanisms and outcomes",
                methodology="Quantitative experimental design with statistical analysis",
                timeline_estimate="12-18 months",
                resource_requirements=["Research participants/data", "Statistical software", "Laboratory/field access"],
                expected_impact="Medium - contributes to empirical evidence base",
                difficulty_level="moderate",
                interdisciplinary_connections=[self._determine_primary_domain(topic), "data_science"],
                potential_pitfalls=["Sample size limitations", "Measurement validity", "External validity"],
                success_metrics=["Statistical significance", "Effect size", "Reproducibility"]
            )
            directions.append(direction)
        
        return directions
    
    def _generate_theoretical_directions(self, topic: str, context: str) -> List[ResearchDirection]:
        """Generate theoretical research directions"""
        directions = []
        concepts = self._extract_key_concepts(topic + " " + context)
        
        for concept in concepts[:2]:
            direction = ResearchDirection(
                title=f"Theoretical Framework Development for {concept}",
                description=f"Development of comprehensive theoretical model explaining {concept} phenomena",
                methodology="Literature review, conceptual modeling, and theoretical synthesis",
                timeline_estimate="8-12 months",
                resource_requirements=["Literature access", "Modeling software", "Expert consultation"],
                expected_impact="High - provides theoretical foundation for future research",
                difficulty_level="challenging",
                interdisciplinary_connections=[self._determine_primary_domain(topic), "innovation_theory"],
                potential_pitfalls=["Overgeneralization", "Limited empirical validation", "Complexity management"],
                success_metrics=["Model coherence", "Predictive power", "Adoption by field"]
            )
            directions.append(direction)
        
        return directions
    
    def _generate_applied_directions(self, topic: str, context: str) -> List[ResearchDirection]:
        """Generate applied research directions"""
        directions = []
        concepts = self._extract_key_concepts(topic + " " + context)
        domain = self._determine_primary_domain(topic)
        
        for concept in concepts[:2]:
            direction = ResearchDirection(
                title=f"Applied {concept} Solution Development",
                description=f"Development and testing of practical {concept} applications in {domain}",
                methodology="Design-based research with iterative prototyping and user testing",
                timeline_estimate="15-24 months",
                resource_requirements=["Development resources", "User access", "Testing facilities"],
                expected_impact="High - direct practical applications",
                difficulty_level="challenging",
                interdisciplinary_connections=[domain, "systems_engineering", "ai_ml"],
                potential_pitfalls=["Technology limitations", "User adoption", "Scalability challenges"],
                success_metrics=["User satisfaction", "Performance metrics", "Adoption rates"]
            )
            directions.append(direction)
        
        return directions
    
    def _generate_interdisciplinary_directions(self, topic: str, context: str) -> List[ResearchDirection]:
        """Generate interdisciplinary research directions"""
        directions = []
        primary_domain = self._determine_primary_domain(topic)
        
        # Cross-domain connections
        for domain, expertise in self.domain_expertise.items():
            if domain != primary_domain and expertise > 0.6:
                direction = ResearchDirection(
                    title=f"Interdisciplinary Integration: {primary_domain.replace('_', ' ').title()} and {domain.replace('_', ' ').title()}",
                    description=f"Exploring synergies between {primary_domain} and {domain} for innovative solutions",
                    methodology="Cross-disciplinary collaboration with mixed-methods approach",
                    timeline_estimate="18-30 months",
                    resource_requirements=["Multi-disciplinary team", "Cross-domain expertise", "Collaborative tools"],
                    expected_impact="Very High - potential breakthrough insights",
                    difficulty_level="very challenging",
                    interdisciplinary_connections=[primary_domain, domain],
                    potential_pitfalls=["Communication barriers", "Methodology conflicts", "Coordination complexity"],
                    success_metrics=["Novel insights", "Cross-domain impact", "Innovation metrics"]
                )
                directions.append(direction)
                break  # Limit to one interdisciplinary direction for now
        
        return directions
    
    def _score_research_direction(self, direction: ResearchDirection) -> float:
        """Score a research direction for ranking"""
        score = 0.0
        
        # Impact scoring
        impact_scores = {
            "Very High": 1.0,
            "High": 0.8,
            "Medium": 0.6,
            "Low": 0.4
        }
        score += impact_scores.get(direction.expected_impact, 0.5) * 0.4
        
        # Difficulty scoring (inverse - easier is better for ranking)
        difficulty_scores = {
            "easy": 1.0,
            "moderate": 0.8,
            "challenging": 0.6,
            "very challenging": 0.4
        }
        score += difficulty_scores.get(direction.difficulty_level, 0.5) * 0.3
        
        # Interdisciplinary bonus
        interdisciplinary_bonus = min(len(direction.interdisciplinary_connections) * 0.1, 0.3)
        score += interdisciplinary_bonus
        
        return min(score, 1.0)
    
    def _classify_research_direction_type(self, direction: Dict[str, Any]) -> str:
        """Classify the type of a research direction"""
        methodology = direction.get('methodology', '').lower()
        title = direction.get('title', '').lower()
        
        if 'empirical' in methodology or 'experimental' in methodology or 'statistical' in methodology:
            return 'empirical'
        elif 'theoretical' in methodology or 'framework' in title or 'model' in methodology:
            return 'theoretical'
        elif 'applied' in title or 'solution' in title or 'development' in methodology:
            return 'applied'
        elif 'interdisciplinary' in title or 'cross' in methodology:
            return 'interdisciplinary'
        else:
            return 'mixed'
    
    def _filter_by_constraints(self, directions: List[Dict[str, Any]], constraints: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Filter research directions based on resource constraints"""
        filtered = []
        
        max_timeline = constraints.get('max_timeline_months', 36)
        max_difficulty = constraints.get('max_difficulty', 'very challenging')
        required_resources = constraints.get('available_resources', [])
        
        difficulty_order = ['easy', 'moderate', 'challenging', 'very challenging']
        max_difficulty_index = difficulty_order.index(max_difficulty) if max_difficulty in difficulty_order else 3
        
        for direction in directions:
            # Timeline constraint
            timeline_str = direction.get('timeline_estimate', '12 months')
            timeline_months = int(timeline_str.split('-')[0]) if '-' in timeline_str else 12
            if timeline_months > max_timeline:
                continue
            
            # Difficulty constraint
            direction_difficulty = direction.get('difficulty_level', 'moderate')
            if direction_difficulty in difficulty_order:
                direction_index = difficulty_order.index(direction_difficulty)
                if direction_index > max_difficulty_index:
                    continue
            
            # Resource constraint check (simplified)
            direction_resources = direction.get('resource_requirements', [])
            if required_resources:
                has_required = any(req.lower() in ' '.join(direction_resources).lower() 
                                 for req in required_resources)
                if not has_required:
                    continue
            
            filtered.append(direction)
        
        return filtered
    
    def _calculate_avg_difficulty(self, directions: List[Dict[str, Any]]) -> float:
        """Calculate average difficulty score for directions"""
        if not directions:
            return 0.0
        
        difficulty_scores = {
            'easy': 0.25,
            'moderate': 0.5,
            'challenging': 0.75,
            'very challenging': 1.0
        }
        
        total_score = sum(difficulty_scores.get(d.get('difficulty_level', 'moderate'), 0.5) 
                         for d in directions)
        
        return total_score / len(directions)
    
    async def _generate_detailed_insights(self, topic: str, context: str) -> Dict[str, Any]:
        """Generate detailed insights for comprehensive analysis"""
        insights = {
            'deep_analysis': await self._perform_deep_analysis(topic, context),
            'comparative_studies': await self._identify_comparative_opportunities(topic),
            'innovation_pathways': await self._map_innovation_pathways(topic),
            'risk_assessment': await self._assess_research_risks(topic, context),
            'collaboration_opportunities': await self._identify_collaboration_opportunities(topic)
        }
        
        return insights
    
    async def _perform_deep_analysis(self, topic: str, context: str) -> Dict[str, Any]:
        """Perform deep analysis of the research topic"""
        evidence = await self._synthesize_evidence(topic)
        
        return {
            'depth_score': min(len(evidence.get('scraped_articles', [])) / 10.0, 1.0),
            'complexity_indicators': self._identify_complexity_indicators(topic, context),
            'theoretical_depth': self._assess_theoretical_depth(topic),
            'empirical_foundation': len(evidence.get('scraped_articles', [])),
            'synthesis_quality': evidence.get('confidence_level', 0.5)
        }
    
    def _identify_complexity_indicators(self, topic: str, context: str) -> List[str]:
        """Identify complexity indicators in the research topic"""
        indicators = []
        text = (topic + " " + context).lower()
        
        complexity_terms = [
            'system', 'integration', 'interaction', 'dynamic', 'emergent',
            'nonlinear', 'multifaceted', 'interdisciplinary', 'multi-scale'
        ]
        
        for term in complexity_terms:
            if term in text:
                indicators.append(f"Complex {term} relationships")
        
        return indicators[:5]
    
    def _assess_theoretical_depth(self, topic: str) -> str:
        """Assess the theoretical depth available for the topic"""
        concepts = self._extract_key_concepts(topic)
        
        if len(concepts) > 8:
            return 'deep'
        elif len(concepts) > 4:
            return 'moderate'
        else:
            return 'shallow'
    
    async def _identify_comparative_opportunities(self, topic: str) -> List[Dict[str, Any]]:
        """Identify opportunities for comparative studies"""
        evidence = await self._synthesize_evidence(topic)
        opportunities = []
        
        # Compare entities
        entities = evidence.get('entity_insights', [])
        if len(entities) > 1:
            for i, entity_a in enumerate(entities[:3]):
                for entity_b in entities[i+1:4]:
                    opportunities.append({
                        'comparison_type': 'entity_comparison',
                        'subject_a': entity_a['name'],
                        'subject_b': entity_b['name'],
                        'potential_insights': f"Comparative analysis of {entity_a['name']} vs {entity_b['name']}",
                        'feasibility': 'high' if entity_a.get('mention_count', 0) > 2 and entity_b.get('mention_count', 0) > 2 else 'moderate'
                    })
        
        return opportunities[:4]
    
    async def _map_innovation_pathways(self, topic: str) -> Dict[str, Any]:
        """Map potential innovation pathways"""
        evidence = await self._synthesize_evidence(topic)
        
        pathways = {
            'technology_transfer': [],
            'process_innovation': [],
            'system_integration': [],
            'breakthrough_potential': []
        }
        
        # Analyze reports for innovation opportunities
        for report in evidence.get('generated_reports', []):
            opportunities = report.get('opportunities', [])
            for opp in opportunities[:2]:
                if 'technology' in opp.lower():
                    pathways['technology_transfer'].append(opp)
                elif 'process' in opp.lower():
                    pathways['process_innovation'].append(opp)
                elif 'system' in opp.lower() or 'integration' in opp.lower():
                    pathways['system_integration'].append(opp)
                else:
                    pathways['breakthrough_potential'].append(opp)
        
        return pathways
    
    async def _assess_research_risks(self, topic: str, context: str) -> Dict[str, Any]:
        """Assess research risks and mitigation strategies"""
        evidence = await self._synthesize_evidence(topic)
        
        risks = {
            'technical_risks': [],
            'resource_risks': [],
            'market_risks': [],
            'methodological_risks': [],
            'mitigation_strategies': []
        }
        
        # Extract risks from reports
        for report in evidence.get('generated_reports', []):
            report_risks = report.get('risks', [])
            for risk in report_risks[:2]:
                if 'technical' in risk.lower() or 'technology' in risk.lower():
                    risks['technical_risks'].append(risk)
                elif 'resource' in risk.lower() or 'cost' in risk.lower():
                    risks['resource_risks'].append(risk)
                elif 'market' in risk.lower() or 'adoption' in risk.lower():
                    risks['market_risks'].append(risk)
                else:
                    risks['methodological_risks'].append(risk)
        
        # Add mitigation strategies
        risks['mitigation_strategies'] = [
            'Iterative prototyping to reduce technical risks',
            'Collaborative partnerships to share resource burden',
            'User-centered design to improve market acceptance',
            'Mixed-methods approach to enhance methodological rigor'
        ]
        
        return risks
    
    async def _identify_collaboration_opportunities(self, topic: str) -> List[Dict[str, Any]]:
        """Identify potential collaboration opportunities"""
        evidence = await self._synthesize_evidence(topic)
        opportunities = []
        
        # Academic collaborations based on entities
        academic_entities = [e for e in evidence.get('entity_insights', []) 
                           if 'university' in e.get('description', '').lower() or 
                           'research' in e.get('description', '').lower()]
        
        for entity in academic_entities[:3]:
            opportunities.append({
                'collaboration_type': 'academic',
                'partner': entity['name'],
                'potential_value': f"Research expertise in {entity.get('space_sector', 'relevant domain')}",
                'engagement_level': 'high' if entity.get('mention_count', 0) > 5 else 'moderate'
            })
        
        # Industry collaborations
        industry_entities = [e for e in evidence.get('entity_insights', []) 
                           if e.get('is_space_related') and 'company' in e.get('description', '').lower()]
        
        for entity in industry_entities[:2]:
            opportunities.append({
                'collaboration_type': 'industry',
                'partner': entity['name'],
                'potential_value': f"Industry application in {entity.get('space_sector', 'space technology')}",
                'engagement_level': 'high' if entity.get('mention_count', 0) > 3 else 'moderate'
            })
        
        return opportunities[:6]
    
    async def _get_ml_source_recommendations(self, topic: str, context: str = "") -> Dict[str, Any]:
        """Get ML-powered source recommendations for research topic"""
        if not self.ml_recommender:
            logger.warning("ML Source Recommender not available")
            return {
                'status': 'unavailable',
                'message': 'ML source recommendation service not initialized',
                'fallback_recommendations': self._get_fallback_source_recommendations(topic, context)
            }
        
        try:
            logger.info(f"Getting ML source recommendations for topic: {topic}")
            
            # Determine research type based on topic analysis
            research_type = self._determine_research_type(topic, context)
            
            # Get ML-powered source recommendations with optimal search strategy
            search_strategy = await self.ml_recommender.recommend_sources(
                topic=topic,
                context=context,
                research_type=research_type,
                time_constraint=60  # Default 60-minute research session
            )
            
            # Combine all recommended sources
            all_sources = search_strategy.primary_sources + search_strategy.secondary_sources
            
            # Convert to JSON-serializable format
            recommendations = {
                'status': 'success',
                'search_strategy': {
                    'primary_sources': [
                        {
                            'source_name': source.source_name,
                            'source_url': source.source_url,
                            'source_type': source.source_type,
                            'relevance_score': source.relevance_score,
                            'confidence_score': source.confidence_score,
                            'specialization': source.specialization,
                            'search_strategy': source.search_strategy,
                            'expected_results': source.expected_results,
                            'reasoning': source.reasoning
                        } for source in search_strategy.primary_sources
                    ],
                    'secondary_sources': [
                        {
                            'source_name': source.source_name,
                            'source_url': source.source_url,
                            'source_type': source.source_type,
                            'relevance_score': source.relevance_score,
                            'confidence_score': source.confidence_score,
                            'specialization': source.specialization,
                            'search_strategy': source.search_strategy,
                            'expected_results': source.expected_results,
                            'reasoning': source.reasoning
                        } for source in search_strategy.secondary_sources
                    ],
                    'search_sequence': search_strategy.search_sequence,
                    'time_allocation': search_strategy.time_allocation,
                    'ml_confidence': search_strategy.ml_confidence,
                    'total_expected_results': search_strategy.total_expected_results
                },
                'source_insights': {
                    'primary_sources': [s.source_name for s in search_strategy.primary_sources if s.source_type == 'academic'],
                    'news_sources': [s.source_name for s in all_sources if s.source_type == 'news'],
                    'technical_sources': [s.source_name for s in all_sources if s.source_type == 'technical'],
                    'social_sources': [s.source_name for s in all_sources if s.source_type == 'social'],
                    'government_sources': [s.source_name for s in all_sources if s.source_type == 'government']
                },
                'research_recommendations': {
                    'start_with': all_sources[0].source_name if all_sources else 'No sources available',
                    'high_value_sources': [s.source_name for s in all_sources if s.relevance_score > 0.7],
                    'quick_overview_sources': [s.source_name for s in all_sources if s.confidence_score > 0.8],
                    'deep_dive_sources': [s.source_name for s in all_sources if s.source_type == 'academic']
                },
                'ml_insights': {
                    'topic_classification': research_type,
                    'predicted_research_depth': self._predict_research_depth(topic, context),
                    'interdisciplinary_potential': self._assess_interdisciplinary_potential(topic),
                    'novelty_score': self._assess_topic_novelty(topic, context)
                }
            }
            
            logger.info(f"Generated {len(all_sources)} ML source recommendations ({len(search_strategy.primary_sources)} primary, {len(search_strategy.secondary_sources)} secondary)")
            return recommendations
            
        except Exception as e:
            logger.error(f"Error getting ML source recommendations: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'fallback_recommendations': self._get_fallback_source_recommendations(topic, context)
            }
    
    def _determine_research_type(self, topic: str, context: str) -> str:
        """Determine the research type based on topic analysis"""
        text = (topic + " " + context).lower()
        
        if any(term in text for term in ['review', 'survey', 'overview', 'comprehensive']):
            return 'comprehensive'
        elif any(term in text for term in ['quick', 'fast', 'summary', 'brief']):
            return 'quick'
        elif any(term in text for term in ['deep', 'detailed', 'thorough', 'extensive']):
            return 'deep_dive'
        elif any(term in text for term in ['current', 'latest', 'news', 'recent']):
            return 'current_events'
        elif any(term in text for term in ['technical', 'engineering', 'implementation']):
            return 'technical'
        else:
            return 'standard'
    
    def _get_fallback_source_recommendations(self, topic: str, context: str) -> Dict[str, Any]:
        """Provide fallback source recommendations when ML system is unavailable"""
        domain = self._determine_primary_domain(topic)
        
        fallback_sources = {
            'academic': ['Google Scholar', 'PubMed', 'arXiv', 'IEEE Xplore'],
            'news': ['Science Daily', 'Nature News', 'MIT Technology Review', 'Ars Technica'],
            'technical': ['Stack Overflow', 'GitHub', 'Technical Forums', 'Documentation Sites'],
            'government': ['NASA', 'NIST', 'Government Publications', 'Official Reports'],
            'social': ['Reddit', 'Twitter', 'LinkedIn', 'Professional Networks']
        }
        
        recommendations = []
        for source_type, sources in fallback_sources.items():
            for source in sources[:2]:  # Limit to top 2 per category
                recommendations.append({
                    'source_name': source,
                    'source_type': source_type,
                    'relevance_score': 0.6,  # Default moderate relevance
                    'access_difficulty': 0.3,  # Assume relatively accessible
                    'recommended_time': 15  # Default 15 minutes per source
                })
        
        return {
            'recommended_sources': recommendations,
            'total_sources': len(recommendations),
            'note': 'Fallback recommendations - ML system unavailable'
        }
    
    def _predict_research_depth(self, topic: str, context: str) -> str:
        """Predict required research depth based on topic complexity"""
        complexity_score = self._assess_complexity(topic, context)
        
        if complexity_score > 0.8:
            return 'deep'
        elif complexity_score > 0.5:
            return 'moderate'
        else:
            return 'surface'
    
    def _assess_interdisciplinary_potential(self, topic: str) -> float:
        """Assess potential for interdisciplinary research"""
        interdisciplinary_terms = ['multi', 'cross', 'inter', 'hybrid', 'integrated', 'systems']
        text = topic.lower()
        
        score = sum(0.2 for term in interdisciplinary_terms if term in text)
        
        # Boost score for topics that span multiple domains
        domain_count = sum(1 for domain in self.domain_expertise.keys() if domain.replace('_', ' ') in text)
        score += domain_count * 0.1
        
        return min(score, 1.0)
    
    def _assess_topic_novelty(self, topic: str, context: str) -> float:
        """Assess novelty of the research topic"""
        novel_terms = ['new', 'novel', 'emerging', 'cutting-edge', 'breakthrough', 'innovative', 'revolutionary']
        text = (topic + " " + context).lower()
        
        novelty_score = sum(0.15 for term in novel_terms if term in text)
        
        # Check for recent years mentioned
        import re
        recent_years = re.findall(r'/b20[2-9][0-9]/b', text)
        if recent_years:
            latest_year = max(int(year) for year in recent_years)
            current_year = datetime.now().year
            if current_year - latest_year <= 2:
                novelty_score += 0.3
        
        return min(novelty_score, 1.0)

# Create global instance
theorycrafting_service = TheoryCraftingService()